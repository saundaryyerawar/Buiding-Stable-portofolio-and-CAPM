{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import packages\n",
    "from time import time\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quandl\n",
    "\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#setting figure size\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "\n",
    "#importing required libraries\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler, Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from fastai.structured import add_datepart\n",
    "\n",
    "#to get stock OHCL data\n",
    "from stocker import Stocker\n",
    "\n",
    "quandl.ApiConfig.api_key = '<YOUR-API-KEY>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify Keras is using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500 = ['MMM', 'ABT', 'ABBV', 'ABMD', 'ACN', 'ATVI', 'ADBE', 'AMD', 'AAP', 'AES', 'AMG', 'AFL', 'A', 'APD', 'AKAM', 'ALK', 'ALB', 'ARE', 'ALXN', 'ALGN', 'ALLE', 'AGN', 'ADS', 'LNT', 'ALL', 'GOOGL', 'GOOG', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AWK', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'ANSS', 'ANTM', 'AON', 'AOS', 'APA', 'AIV', 'AAPL', 'AMAT', 'APTV', 'ADM', 'ARNC', 'ANET', 'AJG', 'AIZ', 'ATO', 'T', 'ADSK', 'ADP', 'AZO', 'AVB', 'AVY', 'BHGE', 'BLL', 'BAC', 'BK', 'BAX', 'BBT', 'BDX', 'BRK.B', 'BBY', 'BIIB', 'BLK', 'HRB', 'BA', 'BKNG', 'BWA', 'BXP', 'BSX', 'BHF', 'BMY', 'AVGO', 'BR', 'BF.B', 'CHRW', 'COG', 'CDNS', 'CPB', 'COF', 'CPRI', 'CAH', 'KMX', 'CCL', 'CAT', 'CBOE', 'CBRE', 'CBS', 'CE', 'CELG', 'CNC', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHTR', 'CVX', 'CMG', 'CB', 'CHD', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CFG', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'CXO', 'COP', 'ED', 'STZ', 'COO', 'CPRT', 'GLW', 'COST', 'COTY', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'FANG', 'DLR', 'DFS', 'DISCA', 'DISCK', 'DISH', 'DG', 'DLTR', 'D', 'DOV', 'DWDP', 'DTE', 'DRE', 'DUK', 'DXC', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ETR', 'EOG', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'EVRG', 'ES', 'RE', 'EXC', 'EXPE', 'EXPD', 'EXR', 'XOM', 'FFIV', 'FB', 'FAST', 'FRT', 'FDX', 'FIS', 'FITB', 'FE', 'FRC', 'FISV', 'FLT', 'FLIR', 'FLS', 'FLR', 'FMC', 'FL', 'F', 'FTNT', 'FTV', 'FBHS', 'BEN', 'FCX', 'GPS', 'GRMN', 'IT', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GILD', 'GPN', 'GS', 'GWW', 'HAL', 'HBI', 'HOG', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HSIC', 'HSY', 'HES', 'HPE', 'HLT', 'HFC', 'HOLX', 'HD', 'HON', 'HRL', 'HST', 'HPQ', 'HUM', 'HBAN', 'HII', 'IDXX', 'INFO', 'ITW', 'ILMN', 'IR', 'INTC', 'ICE', 'IBM', 'INCY', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IPGP', 'IQV', 'IRM', 'JKHY', 'JEC', 'JBHT', 'JEF', 'SJM', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KEYS', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KHC', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LW', 'LEG', 'LEN', 'LLY', 'LNC', 'LIN', 'LKQ', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MXIM', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MTD', 'MGM', 'MCHP', 'MU', 'MSFT', 'MAA', 'MHK', 'TAP', 'MDLZ', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MSCI', 'MYL', 'NDAQ', 'NOV', 'NKTR', 'NTAP', 'NFLX', 'NWL', 'NEM', 'NWSA', 'NWS', 'NEE', 'NLSN', 'NKE', 'NI', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NCLH', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'PCAR', 'PKG', 'PH', 'PAYX', 'PYPL', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PM', 'PSX', 'PNW', 'PXD', 'PNC', 'RL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RJF', 'RTN', 'O', 'RHT', 'REG', 'REGN', 'RF', 'RSG', 'RMD', 'RHI', 'ROK', 'ROL', 'ROP', 'ROST', 'RCL', 'CRM', 'SBAC', 'SLB', 'STX', 'SEE', 'SRE', 'SHW', 'SPG', 'SWKS', 'SLG', 'SNA', 'SO', 'LUV', 'SPGI', 'SWK', 'SBUX', 'STT', 'SYK', 'STI', 'SIVB', 'SYMC', 'SYF', 'SNPS', 'SYY', 'TROW', 'TTWO', 'TPR', 'TGT', 'TEL', 'FTI', 'TFX', 'TXN', 'TXT', 'TMO', 'TIF', 'TWTR', 'TJX', 'TMK', 'TSS', 'TSCO', 'TDG', 'TRV', 'TRIP', 'FOXA', 'FOX', 'TSN', 'UDR', 'ULTA', 'USB', 'UAA', 'UA', 'UNP', 'UAL', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VRSK', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WAB', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'WEC', 'WCG', 'WFC', 'WELL', 'WDC', 'WU', 'WRK', 'WY', 'WHR', 'WMB', 'WLTW', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XYL', 'YUM', 'ZBH', 'ZION', 'ZTS']\n",
    "best100 = ['msci', 'aig', 'cf', 'dte', 'gww', 're', 'apa', 'ual', 'ndaq', 'amp', 'pru', 'pcar', 'csx', 'dre', 'sna', 'rtn', 'ma', 'usb', 'o', 'hon', 'antm', 'wat', 'tfx', 'akam', 'sti', 'mnst', 'ads', 'dva', 'pep', 'wynn', 'mdlz', 'regn', 'incy', 'pnr', 'etr', 'tss', 'duk', 'mtb', 'ksu', 'fcx', 'aee', 'wfc', 'flir', 'sbac', 'vno', 'mat', 'zion', 'nwl', 'gd', 'amgn', 'mac', 'oxy', 'ed', 'sre', 'blk', 'pld', 'lmt', 'xel', 'gild', 'ctl', 'vlo', 'aes', 'aos', 'arnc', 'eqix', 'cof', 'k', 'es', 'dhr', 'hsy', 'pgr', 'irm', 'udr', 'amzn', 'bk', 'mmc', 'hcp', 'ppl', 'tgt', 'fmc', 'stt', 'ba', 'mcd', 'cme', 'ivz', 'axp', 'intc', 'xom', 'idxx', 'ess', 'amg', 'afl', 'ups', 'mo', 'omc', 'hum', 'swk', 'unm', 'cern', 'ce', 'unp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 years data\n",
    "period = 365 * 10\n",
    "# period of 10 years from 12-31-2017 backwards go back to 01-04-2008\n",
    "date_range = ('01-01-2008', '12-31-2017')\n",
    "max_stocker = '03-27-2018'\n",
    "min_date = datetime.datetime.strptime(date_range[0], \"%m-%d-%Y\")\n",
    "max_date = datetime.datetime.strptime(max_stocker, \"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSCI Stocker Initialized. Data covers 2007-11-15 to 2018-03-27.\n",
      "AIG Stocker Initialized. Data covers 1984-09-07 to 2018-03-27.\n",
      "CF Stocker Initialized. Data covers 2005-08-11 to 2018-03-27.\n",
      "DTE Stocker Initialized. Data covers 1970-01-02 to 2018-03-27.\n",
      "GWW Stocker Initialized. Data covers 1984-12-17 to 2018-03-27.\n",
      "RE Stocker Initialized. Data covers 1995-10-03 to 2018-03-27.\n",
      "APA Stocker Initialized. Data covers 1979-05-15 to 2018-03-27.\n",
      "UAL Stocker Initialized. Data covers 2006-02-06 to 2018-03-27.\n",
      "NDAQ Stocker Initialized. Data covers 2002-07-01 to 2018-03-27.\n",
      "AMP Stocker Initialized. Data covers 2005-09-15 to 2018-03-27.\n",
      "PRU Stocker Initialized. Data covers 2001-12-13 to 2018-03-27.\n",
      "PCAR Stocker Initialized. Data covers 1986-07-09 to 2018-03-27.\n",
      "CSX Stocker Initialized. Data covers 1980-11-03 to 2018-03-27.\n",
      "DRE Stocker Initialized. Data covers 1987-11-05 to 2018-03-27.\n",
      "SNA Stocker Initialized. Data covers 1985-07-01 to 2018-03-27.\n",
      "RTN Stocker Initialized. Data covers 1981-12-31 to 2018-03-27.\n",
      "MA Stocker Initialized. Data covers 2006-05-25 to 2018-03-27.\n",
      "USB Stocker Initialized. Data covers 1987-11-05 to 2018-03-27.\n",
      "O Stocker Initialized. Data covers 1994-10-18 to 2018-03-27.\n",
      "HON Stocker Initialized. Data covers 1970-01-02 to 2018-03-27.\n",
      "ANTM Stocker Initialized. Data covers 2001-10-30 to 2018-03-27.\n",
      "WAT Stocker Initialized. Data covers 1995-11-17 to 2018-03-27.\n",
      "TFX Stocker Initialized. Data covers 1988-02-18 to 2018-03-27.\n",
      "AKAM Stocker Initialized. Data covers 1999-10-29 to 2018-03-27.\n",
      "STI Stocker Initialized. Data covers 1987-12-30 to 2018-03-27.\n",
      "MNST Stocker Initialized. Data covers 1995-08-18 to 2018-03-27.\n",
      "ADS Stocker Initialized. Data covers 2001-06-15 to 2018-03-27.\n",
      "DVA Stocker Initialized. Data covers 1995-10-31 to 2018-03-27.\n",
      "PEP Stocker Initialized. Data covers 1972-06-01 to 2018-03-27.\n",
      "WYNN Stocker Initialized. Data covers 2002-10-25 to 2018-03-27.\n",
      "MDLZ Stocker Initialized. Data covers 2001-06-13 to 2018-03-27.\n",
      "REGN Stocker Initialized. Data covers 1991-04-02 to 2018-03-27.\n",
      "INCY Stocker Initialized. Data covers 1993-11-04 to 2018-03-27.\n",
      "PNR Stocker Initialized. Data covers 1973-05-03 to 2018-03-27.\n",
      "ETR Stocker Initialized. Data covers 1972-06-01 to 2018-03-27.\n",
      "TSS Stocker Initialized. Data covers 1989-06-30 to 2018-03-27.\n",
      "DUK Stocker Initialized. Data covers 1983-04-06 to 2018-03-27.\n",
      "MTB Stocker Initialized. Data covers 1991-10-04 to 2018-03-27.\n",
      "KSU Stocker Initialized. Data covers 1987-11-05 to 2018-03-27.\n",
      "FCX Stocker Initialized. Data covers 1995-07-10 to 2018-03-27.\n",
      "AEE Stocker Initialized. Data covers 1998-01-02 to 2018-03-27.\n",
      "WFC Stocker Initialized. Data covers 1972-06-01 to 2018-03-27.\n",
      "FLIR Stocker Initialized. Data covers 1993-06-22 to 2018-03-27.\n",
      "SBAC Stocker Initialized. Data covers 1999-06-16 to 2018-03-27.\n",
      "VNO Stocker Initialized. Data covers 1988-01-05 to 2018-03-27.\n",
      "MAT Stocker Initialized. Data covers 1982-01-04 to 2018-03-27.\n",
      "ZION Stocker Initialized. Data covers 1990-03-26 to 2018-03-27.\n",
      "NWL Stocker Initialized. Data covers 1984-07-19 to 2018-03-27.\n",
      "GD Stocker Initialized. Data covers 1977-01-03 to 2018-03-27.\n",
      "AMGN Stocker Initialized. Data covers 1984-09-07 to 2018-03-27.\n"
     ]
    }
   ],
   "source": [
    "tickers = best100[:50]\n",
    "tickerobjs = {} \n",
    "for ticker in tickers:\n",
    "    tickerobjs[ticker] = (Stocker(ticker=ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 msci <stocker.Stocker object at 0x7f1a5c0286d8>\n",
      "1 aig <stocker.Stocker object at 0x7f1adabd9518>\n",
      "2 cf <stocker.Stocker object at 0x7f1a7406d4a8>\n",
      "3 dte <stocker.Stocker object at 0x7f1a74063fd0>\n",
      "4 gww <stocker.Stocker object at 0x7f1a4852e898>\n",
      "5 re <stocker.Stocker object at 0x7f1a758dd0f0>\n",
      "6 apa <stocker.Stocker object at 0x7f1a4856b668>\n",
      "7 ual <stocker.Stocker object at 0x7f1a4853eac8>\n",
      "8 ndaq <stocker.Stocker object at 0x7f1a4853e4a8>\n",
      "9 amp <stocker.Stocker object at 0x7f1af3731fd0>\n",
      "10 pru <stocker.Stocker object at 0x7f1a742d4f98>\n",
      "11 pcar <stocker.Stocker object at 0x7f1a5c028400>\n",
      "12 csx <stocker.Stocker object at 0x7f1a4853e860>\n",
      "13 dre <stocker.Stocker object at 0x7f1a4853e400>\n",
      "14 sna <stocker.Stocker object at 0x7f1a4853d668>\n",
      "15 rtn <stocker.Stocker object at 0x7f1a4857f940>\n",
      "16 ma <stocker.Stocker object at 0x7f1af3ffeb00>\n",
      "17 usb <stocker.Stocker object at 0x7f1a5c03fc18>\n",
      "18 o <stocker.Stocker object at 0x7f1a4853ec18>\n",
      "19 hon <stocker.Stocker object at 0x7f1a48523e80>\n",
      "20 antm <stocker.Stocker object at 0x7f1a404df240>\n",
      "21 wat <stocker.Stocker object at 0x7f1a48506390>\n",
      "22 tfx <stocker.Stocker object at 0x7f1a485060b8>\n",
      "23 akam <stocker.Stocker object at 0x7f1a485236d8>\n",
      "24 sti <stocker.Stocker object at 0x7f1a485d8b38>\n",
      "25 mnst <stocker.Stocker object at 0x7f1a485d8c18>\n",
      "26 ads <stocker.Stocker object at 0x7f1a485d8668>\n",
      "27 dva <stocker.Stocker object at 0x7f1a485239b0>\n",
      "28 pep <stocker.Stocker object at 0x7f1a485d8dd8>\n",
      "29 wynn <stocker.Stocker object at 0x7f1a4851ac50>\n",
      "30 mdlz <stocker.Stocker object at 0x7f1a4851ae80>\n",
      "31 regn <stocker.Stocker object at 0x7f1a48523f28>\n",
      "32 incy <stocker.Stocker object at 0x7f1a48523b70>\n",
      "33 pnr <stocker.Stocker object at 0x7f1a4851a048>\n",
      "34 etr <stocker.Stocker object at 0x7f1a485068d0>\n",
      "35 tss <stocker.Stocker object at 0x7f1a4851a780>\n",
      "36 duk <stocker.Stocker object at 0x7f1a4851a160>\n",
      "37 mtb <stocker.Stocker object at 0x7f1a4851ac88>\n",
      "38 ksu <stocker.Stocker object at 0x7f1a4851a2b0>\n",
      "39 fcx <stocker.Stocker object at 0x7f1a4851ada0>\n",
      "40 aee <stocker.Stocker object at 0x7f1a4851acc0>\n",
      "41 wfc <stocker.Stocker object at 0x7f1a404df3c8>\n",
      "42 flir <stocker.Stocker object at 0x7f1a404dfef0>\n",
      "43 sbac <stocker.Stocker object at 0x7f1a48506b38>\n",
      "44 vno <stocker.Stocker object at 0x7f1a404c7978>\n",
      "45 mat <stocker.Stocker object at 0x7f1a48506278>\n",
      "46 zion <stocker.Stocker object at 0x7f1a404a9eb8>\n",
      "47 nwl <stocker.Stocker object at 0x7f1a404a9d68>\n",
      "48 gd <stocker.Stocker object at 0x7f1a404a92b0>\n",
      "49 amgn <stocker.Stocker object at 0x7f1a404c7898>\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(tickerobjs.items()): \n",
    "    print(i, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_ciks = pd.read_csv('AllSecTickers.csv')\n",
    "filing_sentiments = {}\n",
    "for ticker in tickers:\n",
    "    cik = int(tickers_ciks.loc[tickers_ciks['ticker'] == ticker]['cik'])\n",
    "    fname = str(cik).zfill(10) + '.csv'\n",
    "    \n",
    "    tenQs = pd.read_csv(\n",
    "        'sentiment-scores/10-Q/{}'.format(fname),\n",
    "        names=['Cik','Coname','Date','Form','Secname','Neg_Score','Neu_Score','Pos_Score']\n",
    "    )\n",
    "\n",
    "    tenKs = pd.read_csv(\n",
    "        'sentiment-scores/10-K/{}'.format(fname),\n",
    "        names=['Cik','Coname','Date','Form','Secname','Neg_Score','Neu_Score','Pos_Score']\n",
    "    )\n",
    "    \n",
    "    sentiments = pd.concat([tenQs, tenKs], ignore_index=True)\n",
    "    sentiments['Date'] = pd.to_datetime(sentiments.Date, format='%Y-%m-%d')\n",
    "    sentiments.index = sentiments['Date']\n",
    "    sentiments = sentiments.sort_index(ascending=True, axis=0)\n",
    "    \n",
    "    filing_sentiments[ticker] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only keep stocks within the specified date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "finals = {}\n",
    "for i, (k, v) in enumerate(tickerobjs.items()):\n",
    "    try:\n",
    "        if v.min_date.date() < min_date.date() and v.max_date.date() <= max_date.date():\n",
    "            finals[k] = v\n",
    "    except AttributeError:\n",
    "        pass\n",
    "print(len(finals))\n",
    "tickerobjs = finals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafay/portfolio-risk-assessment-using-ml/stocker.py:172: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  trim_df = df[(df['Date'] >= start_date.date()) &\n",
      "/home/rafay/portfolio-risk-assessment-using-ml/stocker.py:173: FutureWarning: Comparing Series of datetimes with 'datetime.date'.  Currently, the\n",
      "'datetime.date' is coerced to a datetime. In the future pandas will\n",
      "not coerce, and a TypeError will be raised. To retain the current\n",
      "behavior, convert the 'datetime.date' to a datetime with\n",
      "'pd.Timestamp'.\n",
      "  (df['Date'] <= end_date.date())]\n"
     ]
    }
   ],
   "source": [
    "stocks_data = {}\n",
    "\n",
    "for i, (ticker, stocker) in enumerate(tickerobjs.items()):\n",
    "    df = stocker.make_df(date_range[0], date_range[1])\n",
    "    # ddd date features\n",
    "    add_datepart(df, 'Date', drop=False)\n",
    "    # drop unwanted columns date feature columns\n",
    "    df = df.drop(['Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start', 'Elapsed'], axis=1)\n",
    "    df['Is_month_end'] = df['Is_month_end'].astype(int)\n",
    "    df['Is_month_start'] = df['Is_month_start'].astype(int)\n",
    "    # setting index as date\n",
    "    df['Date'] = pd.to_datetime(df.Date,format='%Y-%m-%d')\n",
    "    df.index = df['Date']\n",
    "    # sort df by date\n",
    "    df = df.sort_index(ascending=True, axis=0)\n",
    "    \n",
    "    stocks_data[ticker] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix length issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list = pd.date_range(start=date_range[0], end=date_range[1], freq='B')\n",
    "date_df = pd.DataFrame(date_list).rename(columns={0: 'Date'})\n",
    "date_df.index = date_df['Date']\n",
    "date_df.rename(columns={'Date': 'Date2'}, inplace=True)\n",
    "\n",
    "for i, (ticker, df) in enumerate(stocks_data.items()):\n",
    "    stocks_data[ticker] = pd.concat([df, date_df], ignore_index=False, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine sentiments data with DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ticker, df) in enumerate(stocks_data.items()):\n",
    "    sentiment_df = filing_sentiments[ticker]\n",
    "    forward_merged = pd.merge_asof(df, sentiment_df, left_index=True, right_index=True, direction='backward')\n",
    "    stocks_data[ticker] = forward_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print train, val and test count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2609, 2087, 261, 261)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_SPLIT = 0.8  # 80% of total dataset\n",
    "VAL_TEST_SPLIT = 0.5  # 50% of the remaining dataset\n",
    "\n",
    "total_count = len(stocks_data[list(stocks_data.keys())[0]])\n",
    "train_count = int(total_count * TRAIN_SPLIT)\n",
    "left = total_count - train_count\n",
    "valid_count = int(left * VAL_TEST_SPLIT)\n",
    "test_count = int(left - valid_count)\n",
    "\n",
    "print(sum([train_count, valid_count, test_count]))\n",
    "total_count, train_count, valid_count, test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new dataset with required columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pick_columns = {\n",
    "    'Adj. Open', 'Adj. High', 'Adj. Low', 'Adj. Close', 'Adj. Volume', 'Month', 'Week', 'Day', 'Dayofweek', \n",
    "    'Dayofyear', 'Is_month_end', 'Is_month_start', 'Neg_Score'\n",
    "}\n",
    "y_column = {'Adj. Close'}\n",
    "x_columns = pick_columns - y_column\n",
    "\n",
    "for i, (ticker, df) in enumerate(stocks_data.items()):\n",
    "    # creating new dataframe\n",
    "\n",
    "    new_data = pd.DataFrame(df, columns=pick_columns).interpolate(limit_direction='both')\n",
    "        \n",
    "    x_df = pd.DataFrame(new_data, columns=x_columns)\n",
    "    y_df = pd.DataFrame(new_data, columns=y_column)\n",
    "    stocks_data[ticker] = {\n",
    "        'x': x_df, 'y': y_df,\n",
    "        'x_train': x_df[0:train_count], 'y_train': y_df[0:train_count],\n",
    "        'x_valid': x_df[train_count:train_count+valid_count], 'y_valid': y_df[train_count:train_count+valid_count],\n",
    "        'x_test': x_df[train_count+valid_count:], 'y_test': y_df[train_count+valid_count:],\n",
    "        'new_data': new_data\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Is_month_start</th>\n",
       "      <th>Adj. Volume</th>\n",
       "      <th>Neg_Score</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Dayofweek</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Is_month_end</th>\n",
       "      <th>Week</th>\n",
       "      <th>Dayofyear</th>\n",
       "      <th>Month</th>\n",
       "      <th>Adj. High</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-04-09</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>835100.0</td>\n",
       "      <td>0.038033</td>\n",
       "      <td>26.925191</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.712252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.262311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-10</th>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159500.0</td>\n",
       "      <td>0.038033</td>\n",
       "      <td>28.993399</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.521168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>28.993399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-11</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173800.0</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>26.934810</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.097907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.887148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-14</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>151000.0</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>26.742419</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.367255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.213778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-04-15</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84500.0</td>\n",
       "      <td>0.030889</td>\n",
       "      <td>26.848234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.741983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>27.059865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Day  Is_month_start  Adj. Volume  Neg_Score  Adj. Open  \\\n",
       "Date                                                                  \n",
       "2008-04-09   9.0             0.0     835100.0   0.038033  26.925191   \n",
       "2008-04-10  10.0             0.0     159500.0   0.038033  28.993399   \n",
       "2008-04-11  11.0             0.0     173800.0   0.030889  26.934810   \n",
       "2008-04-14  14.0             0.0     151000.0   0.030889  26.742419   \n",
       "2008-04-15  15.0             0.0      84500.0   0.030889  26.848234   \n",
       "\n",
       "            Dayofweek   Adj. Low  Is_month_end  Week  Dayofyear  Month  \\\n",
       "Date                                                                     \n",
       "2008-04-09        2.0  23.712252           0.0  15.0      100.0    4.0   \n",
       "2008-04-10        3.0  26.521168           0.0  15.0      101.0    4.0   \n",
       "2008-04-11        4.0  26.097907           0.0  15.0      102.0    4.0   \n",
       "2008-04-14        0.0  26.367255           0.0  16.0      105.0    4.0   \n",
       "2008-04-15        1.0  25.741983           0.0  16.0      106.0    4.0   \n",
       "\n",
       "            Adj. High  \n",
       "Date                   \n",
       "2008-04-09  28.262311  \n",
       "2008-04-10  28.993399  \n",
       "2008-04-11  27.887148  \n",
       "2008-04-14  27.213778  \n",
       "2008-04-15  27.059865  "
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_merged = stocks_data['msci']['x']\n",
    "forward_merged[forward_merged.index >= '2008-04-09'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Train shapes for msci\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for msci\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for msci\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for aig\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for aig\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for aig\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for cf\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for cf\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for cf\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for dte\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for dte\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for dte\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for gww\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for gww\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for gww\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for re\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for re\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for re\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for apa\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for apa\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for apa\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for ual\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for ual\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for ual\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for ndaq\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for ndaq\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for ndaq\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for amp\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for amp\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for amp\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for pru\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for pru\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for pru\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for pcar\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for pcar\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for pcar\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for csx\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for csx\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for csx\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for dre\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for dre\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for dre\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for sna\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for sna\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for sna\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for rtn\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for rtn\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for rtn\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for ma\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for ma\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for ma\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for usb\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for usb\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for usb\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for o\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for o\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for o\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for hon\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for hon\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for hon\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for antm\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for antm\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for antm\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for wat\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for wat\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for wat\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for tfx\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for tfx\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for tfx\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for akam\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for akam\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for akam\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for sti\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for sti\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for sti\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for mnst\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for mnst\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for mnst\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for ads\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for ads\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for ads\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for dva\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for dva\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for dva\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for pep\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for pep\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for pep\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for wynn\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for wynn\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for wynn\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for mdlz\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for mdlz\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for mdlz\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for regn\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for regn\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for regn\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for incy\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for incy\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for incy\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for pnr\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for pnr\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for pnr\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for etr\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for etr\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for etr\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for tss\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for tss\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for tss\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for duk\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for duk\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for duk\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for mtb\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for mtb\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for mtb\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for ksu\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for ksu\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for ksu\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for fcx\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for fcx\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for fcx\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for aee\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for aee\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for aee\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for wfc\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for wfc\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for wfc\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for flir\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for flir\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for flir\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for sbac\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for sbac\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for sbac\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for vno\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for vno\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for vno\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for mat\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for mat\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for mat\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for zion\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for zion\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for zion\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for nwl\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for nwl\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for nwl\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for gd\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for gd\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for gd\n",
      "(261, 12) (261, 1)\n",
      "*****Train shapes for amgn\n",
      "(2087, 12) (2087, 1)\n",
      "*****Validation shapes for amgn\n",
      "(261, 12) (261, 1)\n",
      "*****Test shapes for amgn\n",
      "(261, 12) (261, 1)\n"
     ]
    }
   ],
   "source": [
    "from numpy import hstack\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "for i, (ticker, ds) in enumerate(stocks_data.items()):\n",
    "    x_series = []\n",
    "    y_series = []\n",
    "    x_train_series = []\n",
    "    y_train_series = []\n",
    "    x_valid_series = []\n",
    "    y_valid_series = []\n",
    "    x_test_series = []\n",
    "    y_test_series = []\n",
    "    \n",
    "    ds['x_values'] = ds['x'].values\n",
    "    ds['y_values'] = ds['y'].values\n",
    "    \n",
    "    ds['x_train_values'] = ds['x_train'].values\n",
    "    ds['y_train_values'] = ds['y_train'].values\n",
    "    \n",
    "    ds['x_valid_values'] = ds['x_valid'].values\n",
    "    ds['y_valid_values'] = ds['y_valid'].values\n",
    "    \n",
    "    ds['x_test_values'] = ds['x_test'].values\n",
    "    ds['y_test_values'] = ds['y_test'].values\n",
    "    \n",
    "    # Append values\n",
    "    x_series.append(ds['x_values'])\n",
    "    y_series.append(ds['y_values'])\n",
    "    \n",
    "    x_train_series.append(ds['x_train_values'])\n",
    "    y_train_series.append(ds['y_train_values'])\n",
    "    \n",
    "    x_valid_series.append(ds['x_valid_values'])\n",
    "    y_valid_series.append(ds['y_valid_values'])\n",
    "    \n",
    "    x_test_series.append(ds['x_test_values'])\n",
    "    y_test_series.append(ds['y_test_values'])\n",
    "    \n",
    "\n",
    "    x_dataset = hstack(tuple(x_series))\n",
    "    y_dataset = hstack(tuple(y_series))\n",
    "\n",
    "    x_train_dataset = hstack(tuple(x_train_series))\n",
    "    y_train_dataset = hstack(tuple(y_train_series))\n",
    "\n",
    "    x_valid_dataset = hstack(tuple(x_valid_series))\n",
    "    y_valid_dataset = hstack(tuple(y_valid_series))\n",
    "\n",
    "    x_test_dataset = hstack(tuple(x_test_series))\n",
    "    y_test_dataset = hstack(tuple(y_test_series))\n",
    "\n",
    "    # fit scalers on full series\n",
    "    x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    x_dataset = x_scaler.fit_transform(x_dataset)\n",
    "    y_dataset = y_scaler.fit_transform(y_dataset)\n",
    "    \n",
    "    # Scale train and validation datasets\n",
    "    x_train_dataset = x_scaler.transform(x_train_dataset)\n",
    "    y_train_dataset = y_scaler.transform(y_train_dataset)\n",
    "\n",
    "    x_valid_dataset = x_scaler.transform(x_valid_dataset)\n",
    "    y_valid_dataset = y_scaler.transform(y_valid_dataset)\n",
    "\n",
    "    x_test_dataset = x_scaler.transform(x_test_dataset)\n",
    "    y_test_dataset = y_scaler.transform(y_test_dataset)\n",
    "    \n",
    "    ds['x_train_dataset'] = x_train_dataset\n",
    "    ds['y_train_dataset'] = y_train_dataset\n",
    "    \n",
    "    ds['x_valid_dataset'] = x_valid_dataset\n",
    "    ds['y_valid_dataset'] = y_valid_dataset\n",
    "    \n",
    "    ds['x_test_dataset'] = x_test_dataset\n",
    "    ds['y_test_dataset'] = y_test_dataset\n",
    "    \n",
    "    # Store feature scalers\n",
    "    ds['x_scaler'] = x_scaler\n",
    "    ds['y_scaler'] = y_scaler\n",
    "    \n",
    "    print('*' * 5 + 'Train shapes for ' + ticker)\n",
    "    print(x_train_dataset.shape, y_train_dataset.shape) \n",
    "    \n",
    "    print('*' * 5 + 'Validation shapes for ' + ticker)\n",
    "    print(x_valid_dataset.shape, y_valid_dataset.shape) \n",
    "    \n",
    "    print('*' * 5 + 'Test shapes for ' + ticker)\n",
    "    print(x_test_dataset.shape, y_test_dataset.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a TimeseriesGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples for msci: 111\n",
      "Validation samples for msci: 10\n",
      "Test samples for msci: 10\n",
      "Train samples for aig: 111\n",
      "Validation samples for aig: 10\n",
      "Test samples for aig: 10\n",
      "Train samples for cf: 111\n",
      "Validation samples for cf: 10\n",
      "Test samples for cf: 10\n",
      "Train samples for dte: 111\n",
      "Validation samples for dte: 10\n",
      "Test samples for dte: 10\n",
      "Train samples for gww: 111\n",
      "Validation samples for gww: 10\n",
      "Test samples for gww: 10\n",
      "Train samples for re: 111\n",
      "Validation samples for re: 10\n",
      "Test samples for re: 10\n",
      "Train samples for apa: 111\n",
      "Validation samples for apa: 10\n",
      "Test samples for apa: 10\n",
      "Train samples for ual: 111\n",
      "Validation samples for ual: 10\n",
      "Test samples for ual: 10\n",
      "Train samples for ndaq: 111\n",
      "Validation samples for ndaq: 10\n",
      "Test samples for ndaq: 10\n",
      "Train samples for amp: 111\n",
      "Validation samples for amp: 10\n",
      "Test samples for amp: 10\n",
      "Train samples for pru: 111\n",
      "Validation samples for pru: 10\n",
      "Test samples for pru: 10\n",
      "Train samples for pcar: 111\n",
      "Validation samples for pcar: 10\n",
      "Test samples for pcar: 10\n",
      "Train samples for csx: 111\n",
      "Validation samples for csx: 10\n",
      "Test samples for csx: 10\n",
      "Train samples for dre: 111\n",
      "Validation samples for dre: 10\n",
      "Test samples for dre: 10\n",
      "Train samples for sna: 111\n",
      "Validation samples for sna: 10\n",
      "Test samples for sna: 10\n",
      "Train samples for rtn: 111\n",
      "Validation samples for rtn: 10\n",
      "Test samples for rtn: 10\n",
      "Train samples for ma: 111\n",
      "Validation samples for ma: 10\n",
      "Test samples for ma: 10\n",
      "Train samples for usb: 111\n",
      "Validation samples for usb: 10\n",
      "Test samples for usb: 10\n",
      "Train samples for o: 111\n",
      "Validation samples for o: 10\n",
      "Test samples for o: 10\n",
      "Train samples for hon: 111\n",
      "Validation samples for hon: 10\n",
      "Test samples for hon: 10\n",
      "Train samples for antm: 111\n",
      "Validation samples for antm: 10\n",
      "Test samples for antm: 10\n",
      "Train samples for wat: 111\n",
      "Validation samples for wat: 10\n",
      "Test samples for wat: 10\n",
      "Train samples for tfx: 111\n",
      "Validation samples for tfx: 10\n",
      "Test samples for tfx: 10\n",
      "Train samples for akam: 111\n",
      "Validation samples for akam: 10\n",
      "Test samples for akam: 10\n",
      "Train samples for sti: 111\n",
      "Validation samples for sti: 10\n",
      "Test samples for sti: 10\n",
      "Train samples for mnst: 111\n",
      "Validation samples for mnst: 10\n",
      "Test samples for mnst: 10\n",
      "Train samples for ads: 111\n",
      "Validation samples for ads: 10\n",
      "Test samples for ads: 10\n",
      "Train samples for dva: 111\n",
      "Validation samples for dva: 10\n",
      "Test samples for dva: 10\n",
      "Train samples for pep: 111\n",
      "Validation samples for pep: 10\n",
      "Test samples for pep: 10\n",
      "Train samples for wynn: 111\n",
      "Validation samples for wynn: 10\n",
      "Test samples for wynn: 10\n",
      "Train samples for mdlz: 111\n",
      "Validation samples for mdlz: 10\n",
      "Test samples for mdlz: 10\n",
      "Train samples for regn: 111\n",
      "Validation samples for regn: 10\n",
      "Test samples for regn: 10\n",
      "Train samples for incy: 111\n",
      "Validation samples for incy: 10\n",
      "Test samples for incy: 10\n",
      "Train samples for pnr: 111\n",
      "Validation samples for pnr: 10\n",
      "Test samples for pnr: 10\n",
      "Train samples for etr: 111\n",
      "Validation samples for etr: 10\n",
      "Test samples for etr: 10\n",
      "Train samples for tss: 111\n",
      "Validation samples for tss: 10\n",
      "Test samples for tss: 10\n",
      "Train samples for duk: 111\n",
      "Validation samples for duk: 10\n",
      "Test samples for duk: 10\n",
      "Train samples for mtb: 111\n",
      "Validation samples for mtb: 10\n",
      "Test samples for mtb: 10\n",
      "Train samples for ksu: 111\n",
      "Validation samples for ksu: 10\n",
      "Test samples for ksu: 10\n",
      "Train samples for fcx: 111\n",
      "Validation samples for fcx: 10\n",
      "Test samples for fcx: 10\n",
      "Train samples for aee: 111\n",
      "Validation samples for aee: 10\n",
      "Test samples for aee: 10\n",
      "Train samples for wfc: 111\n",
      "Validation samples for wfc: 10\n",
      "Test samples for wfc: 10\n",
      "Train samples for flir: 111\n",
      "Validation samples for flir: 10\n",
      "Test samples for flir: 10\n",
      "Train samples for sbac: 111\n",
      "Validation samples for sbac: 10\n",
      "Test samples for sbac: 10\n",
      "Train samples for vno: 111\n",
      "Validation samples for vno: 10\n",
      "Test samples for vno: 10\n",
      "Train samples for mat: 111\n",
      "Validation samples for mat: 10\n",
      "Test samples for mat: 10\n",
      "Train samples for zion: 111\n",
      "Validation samples for zion: 10\n",
      "Test samples for zion: 10\n",
      "Train samples for nwl: 111\n",
      "Validation samples for nwl: 10\n",
      "Test samples for nwl: 10\n",
      "Train samples for gd: 111\n",
      "Validation samples for gd: 10\n",
      "Test samples for gd: 10\n",
      "Train samples for amgn: 111\n",
      "Validation samples for amgn: 10\n",
      "Test samples for amgn: 10\n"
     ]
    }
   ],
   "source": [
    "window_length = 90\n",
    "BATCH_SIZE = int(window_length / 5)\n",
    "\n",
    "for i, (ticker, ds) in enumerate(stocks_data.items()):\n",
    "    x_train_dataset = ds['x_train_dataset']\n",
    "    y_train_dataset = ds['y_train_dataset']\n",
    "    \n",
    "    x_valid_dataset = ds['x_valid_dataset']\n",
    "    y_valid_dataset = ds['y_valid_dataset']\n",
    "    \n",
    "    x_test_dataset = ds['x_test_dataset']\n",
    "    y_test_dataset = ds['y_test_dataset']\n",
    "    \n",
    "    train_generator = TimeseriesGenerator(x_train_dataset, y_train_dataset, length=window_length, batch_size=BATCH_SIZE)\n",
    "    print('Train samples for {}: {}'.format(ticker, len(train_generator)))\n",
    "\n",
    "    valid_generator = TimeseriesGenerator(x_valid_dataset, y_valid_dataset, length=window_length, batch_size=BATCH_SIZE)\n",
    "    print('Validation samples for {}: {}'.format(ticker, len(valid_generator)))\n",
    "\n",
    "    test_generator = TimeseriesGenerator(x_test_dataset, y_test_dataset, length=window_length, batch_size=BATCH_SIZE)\n",
    "    print('Test samples for {}: {}'.format(ticker, len(test_generator)))\n",
    "    \n",
    "    ds['train_generator'] = train_generator\n",
    "    ds['valid_generator'] = valid_generator\n",
    "    ds['test_generator'] = test_generator\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****Training for msci\n",
      "Epoch 1/20\n",
      "111/111 [==============================] - 35s 314ms/step - loss: 0.0074 - val_loss: 0.0044\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 25s 223ms/step - loss: 0.0051 - val_loss: 0.0096\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 25s 227ms/step - loss: 0.0053 - val_loss: 0.0063\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 26s 236ms/step - loss: 0.0057 - val_loss: 0.0106\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 27s 243ms/step - loss: 0.0052 - val_loss: 0.0056\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 26s 236ms/step - loss: 0.0053 - val_loss: 0.0059\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 26s 237ms/step - loss: 0.0053 - val_loss: 0.0046\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 26s 232ms/step - loss: 0.0052 - val_loss: 0.0016\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 26s 233ms/step - loss: 0.0056 - val_loss: 0.0015\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 26s 232ms/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 25s 229ms/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0011 - val_loss: 9.2609e-04\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 7.6624e-04 - val_loss: 0.0020\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 25s 227ms/step - loss: 6.3614e-04 - val_loss: 7.6019e-04\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 25s 230ms/step - loss: 5.4861e-04 - val_loss: 7.8205e-04\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 26s 238ms/step - loss: 5.0850e-04 - val_loss: 6.2273e-04\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 25s 226ms/step - loss: 4.6350e-04 - val_loss: 5.5865e-04\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 4.3765e-04 - val_loss: 3.2963e-04\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 25s 229ms/step - loss: 4.5918e-04 - val_loss: 4.5947e-04\n",
      "Val loss for msci: 0.00045946877492640753\n",
      "*****Training for aig\n",
      "Epoch 1/20\n",
      "111/111 [==============================] - 35s 319ms/step - loss: 0.0131 - val_loss: 3.6985e-05\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 26s 232ms/step - loss: 0.0056 - val_loss: 7.0967e-05\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 25s 229ms/step - loss: 9.0687e-04 - val_loss: 4.5286e-05\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 25s 223ms/step - loss: 7.3012e-04 - val_loss: 4.3920e-05\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 26s 236ms/step - loss: 0.0012 - val_loss: 1.6359e-04\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 26s 235ms/step - loss: 0.0010 - val_loss: 1.7379e-05\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0034 - val_loss: 6.1892e-05\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 26s 237ms/step - loss: 5.4646e-04 - val_loss: 2.1634e-05\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 26s 231ms/step - loss: 7.9142e-04 - val_loss: 8.2058e-05\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 6.8835e-04 - val_loss: 1.2487e-05\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 26s 237ms/step - loss: 0.0018 - val_loss: 7.4360e-05\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 26s 230ms/step - loss: 6.0841e-04 - val_loss: 1.1823e-05\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 26s 230ms/step - loss: 0.0018 - val_loss: 1.6717e-04\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0010 - val_loss: 1.3020e-05\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 25s 225ms/step - loss: 0.0033 - val_loss: 6.9041e-05\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 25s 227ms/step - loss: 4.6485e-04 - val_loss: 1.6517e-05\n",
      "Epoch 17/20\n",
      "111/111 [==============================] - 27s 241ms/step - loss: 0.0015 - val_loss: 2.4572e-04\n",
      "Epoch 18/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 0.0010 - val_loss: 1.1185e-05\n",
      "Epoch 19/20\n",
      "111/111 [==============================] - 25s 229ms/step - loss: 0.0030 - val_loss: 1.2165e-04\n",
      "Epoch 20/20\n",
      "111/111 [==============================] - 26s 231ms/step - loss: 4.2229e-04 - val_loss: 1.1529e-05\n",
      "Val loss for aig: 1.1529427099862675e-05\n",
      "*****Training for cf\n",
      "Epoch 1/20\n",
      "111/111 [==============================] - 35s 319ms/step - loss: 0.0220 - val_loss: 0.0844\n",
      "Epoch 2/20\n",
      "111/111 [==============================] - 26s 230ms/step - loss: 0.0131 - val_loss: 0.0974\n",
      "Epoch 3/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0149 - val_loss: 0.0549\n",
      "Epoch 4/20\n",
      "111/111 [==============================] - 26s 230ms/step - loss: 0.0165 - val_loss: 0.0872\n",
      "Epoch 5/20\n",
      "111/111 [==============================] - 26s 238ms/step - loss: 0.0194 - val_loss: 0.0830\n",
      "Epoch 6/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0191 - val_loss: 0.0816\n",
      "Epoch 7/20\n",
      "111/111 [==============================] - 25s 230ms/step - loss: 0.0164 - val_loss: 0.0688\n",
      "Epoch 8/20\n",
      "111/111 [==============================] - 25s 225ms/step - loss: 0.0135 - val_loss: 0.0698\n",
      "Epoch 9/20\n",
      "111/111 [==============================] - 26s 232ms/step - loss: 0.0117 - val_loss: 0.0556\n",
      "Epoch 10/20\n",
      "111/111 [==============================] - 26s 233ms/step - loss: 0.0089 - val_loss: 0.0474\n",
      "Epoch 11/20\n",
      "111/111 [==============================] - 25s 229ms/step - loss: 0.0067 - val_loss: 0.0382\n",
      "Epoch 12/20\n",
      "111/111 [==============================] - 26s 234ms/step - loss: 0.0052 - val_loss: 0.0224\n",
      "Epoch 13/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 0.0043 - val_loss: 0.0170\n",
      "Epoch 14/20\n",
      "111/111 [==============================] - 25s 225ms/step - loss: 0.0037 - val_loss: 0.0150\n",
      "Epoch 15/20\n",
      "111/111 [==============================] - 26s 232ms/step - loss: 0.0034 - val_loss: 0.0159\n",
      "Epoch 16/20\n",
      "111/111 [==============================] - 25s 228ms/step - loss: 0.0033 - val_loss: 0.0150\n",
      "Epoch 17/20\n",
      " 93/111 [========================>.....] - ETA: 4s - loss: 0.0026"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for i, (ticker, ds) in enumerate(stocks_data.items()):\n",
    "    train_generator = ds['train_generator']\n",
    "    valid_generator = ds['valid_generator']\n",
    "    test_generator = ds['test_generator']\n",
    "    \n",
    "    OUTPUT_SIZE = 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(window_length, OUTPUT_SIZE * len(x_columns))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(OUTPUT_SIZE))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    # Tensorboard\n",
    "    tensorboard = TensorBoard(log_dir='logs/{}-{}'.format(ticker, time()))\n",
    "    \n",
    "    # Checkpoint\n",
    "    filepath='modelsneg/{}.weights.best.hdf5'.format(ticker)\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [tensorboard]\n",
    "    \n",
    "    print('*' * 5 + 'Training for {}'.format(ticker))\n",
    "    model.fit_generator(\n",
    "        train_generator, \n",
    "        validation_data=valid_generator, \n",
    "        shuffle=False,\n",
    "        epochs=EPOCHS, \n",
    "        verbose=1, \n",
    "        callbacks=callbacks_list\n",
    "    )\n",
    "    model.save(filepath)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # --Plots while training\n",
    "    val_loss = model.evaluate_generator(valid_generator)\n",
    "    print('Val loss for {}: {}'.format(ticker, val_loss))\n",
    "    \n",
    "    # Make test predictions\n",
    "    test_predict = model.predict_generator(test_generator)\n",
    "    predicted_df = pd.DataFrame(y_scaler.inverse_transform(test_predict))\n",
    "    predicted_df = predicted_df.rename(columns={0: ticker})\n",
    "    \n",
    "    original_test = stocks_data[ticker]['y_test'].iloc[window_length:]\n",
    "    predicted_df.index = original_test.index\n",
    "    \n",
    "    predicted_ticker = pd.concat([predicted_df[ticker], original_test], axis=1)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(32, 20))\n",
    "\n",
    "    plt.plot(predicted_ticker[ticker], linewidth=1, alpha=0.8)\n",
    "    plt.plot(stocks_data[ticker]['y'], linewidth=1, alpha=0.8)\n",
    "    plt.title('Plot for {}, val loss: {}'.format(ticker.upper(), val_loss))\n",
    "    plt.savefig('plotsneg/{}'.format(ticker))\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (ticker, ds) in enumerate(stocks_data.items()):\n",
    "    train_generator = ds['train_generator']\n",
    "    valid_generator = ds['valid_generator']\n",
    "    test_generator = ds['test_generator']\n",
    "    \n",
    "    x_scaler = ds['x_scaler']\n",
    "    y_scaler = ds['y_scaler']\n",
    "    \n",
    "    filepath='models/{}.weights.best.hdf5'.format(ticker)\n",
    "    OUTPUT_SIZE = 1\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(window_length, OUTPUT_SIZE * len(x_columns))))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=50))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(OUTPUT_SIZE))\n",
    "    \n",
    "    model.load_weights(filepath)\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    val_loss = model.evaluate_generator(valid_generator)\n",
    "    print('Val loss for {}: {}'.format(ticker, val_loss))\n",
    "    \n",
    "    # Make test predictions\n",
    "    test_predict = model.predict_generator(test_generator)\n",
    "    predicted_df = pd.DataFrame(y_scaler.inverse_transform(test_predict))\n",
    "    predicted_df = predicted_df.rename(columns={0: ticker})\n",
    "    \n",
    "    original_test = stocks_data[ticker]['y_test'].iloc[window_length:]\n",
    "    predicted_df.index = original_test.index\n",
    "    \n",
    "    predicted_ticker = pd.concat([predicted_df[ticker], original_test], axis=1)\n",
    "    \n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(32, 20))\n",
    "\n",
    "    plt.plot(predicted_ticker[ticker], linewidth=1, alpha=0.8)\n",
    "    plt.plot(stocks_data[ticker]['y'], linewidth=1, alpha=0.8)\n",
    "    plt.title('Plot for {}, val loss: {}'.format(ticker.upper(), val_loss))\n",
    "    plt.savefig('plots/{}'.format(ticker))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
